---
layout: post
title: "机器学习2：常用算法入门小结"
date: 2015-10-19 22:39:54
categories: 机器学习 programming 笔记
excerpt: 机器学习2：常用算法入门小结
---


# 机器学习2：常用算法入门小结


匆匆刷过了著名的机器学习课程(link)，本文记录一下学习经验。当然，这篇文章的前几稿必定不很靠谱。不过，只有经过这些粗糙的文字，才能迭代出更精准的思考。请各位读者不吝赐教。

## 有监督学习 supervised learning

### 线性回归 linear regression
最基本的回归算法。优化对象是 `误差的平方和`。 其实就是我们熟知的最小二乘法。

- 优点：算法和实现极其简单；速度快；low-variance
- 缺点：只能处理线性关系；复杂关系时 high-bias

求解可用梯度下降(gradient descent)或者直接计算(normal equation)。前者需要参数归一化，后者对于大量样本计算量较大。

延伸： 可添加高阶项和相互作用项，但可能遇到的问题是特征数量剧增(特别是相互作用项)。如5个特征的二次项就有5+10+5=20个；而100个特征的二次项就多达100+4950+100=5150个。另一个问题是特征过多可能造成过拟合，可引入正则化系数降低影响。

### 逻辑回归 logistic regression

虽然名字里面有回归，但逻辑回归是一种分类算法。基于sigmoid函数。

- 优点：相对简单，计算量小；可以给出概率解释；
- 缺点：只能进行线性分类。

### 神经网络 neuron network

模拟神经元层级网络的算法结构。基础单元是单个节点，通过简单响应函数的组合可以学习复杂的变量关系（如从基本的与/或/非组成异或等复杂的逻辑关系）。多层的神经网络也被称为深度学习。

- 优点：可学习复杂的非线性关系
- 缺点：实现相对复杂；计算量大；结果的含义无法解释

### 支持向量机 support vector machine

与前面的算法关注整体不同的是，支持向量机更关注分隔界面附近的样本，算法最大化分隔界面与附近的支持向量间的距离。

- 优点：通过核函数可学习非线性关系；
- 缺点：速度慢；

## 无监督学习 unsupervised learning

### k均值 k-means

cluster算法，把样本根据几何距离分成k组（因此各个特征一定要归一化）。

- 优点：易于理解；
- 缺点：只能按距离分组，对于其它类型数据的适应性差

### 主成分分析 principal component analysis

采用正交化变换，把原有数据转换为一组标准正交基，并且按照对方差贡献多少排序。通常前几个主成分就可以解释绝大部分的方差，因此主成分分析常用于减少特征数量（降维）。当数据简化为2-3维时，我们可以有效可视化，对样本产生整体的理解。（可视化在机器学习系统设计初期往往十分重要）

### 协同过滤 collabrating filtering

常用于推荐系统(recommender system)。解决的问题是如何填充某些元素缺失的矩阵（如用户-评分矩阵）。


## 小结：算法之外

### 特征

事实上，在很多机器学习问题之中，不同算法的表现大致相当，决定系统表现的关键往往是`特征设计`。好的特征常比好的算法更重要。另一方面，由于算法大多基于成熟的算法包(如scikit-learn in python)，实际上选择算法并不需要太多时间。针对实际数据调参和特征设计是机器学习中的主要内容。

那么，如何寻找好的特征呢？似乎并没有通用的方法，理解业务目标、理解数据、系统测试，也许是一些可行的方案。特征工程也许需要相当的实际工程经验。

### 机器学习系统的评估

Andrew Ng 在课程中反复提到：机器学习任务中，最宝贵的资源是`工程师的时间`。我们有必要通过各种评估方法，确认自己的算法正在正确的道路上前进。

- 可视化: 我们更擅长观察图形，而非数据，样本量大时更是如此。可视化是我们理解数据和算法效果的最佳工具。
- cross validation: 为了避免过拟合，我们不应在机器学习过程中使用test set的数据。需要优化参数时，应从training set中分出部分数据(常用比例为training set: CV set: test set = 60%:20%:20%)。
- learning curve: 通过比较 J(train) 和 J(test) ，我们可以判断算法是 high bias 还是 high vaviance。
- ceiling analysis: 通过人工标记给出100%正确的中间结果，可判断算法系统流程(pipeline)中的短板。

一些更复杂的算法，如基于决策树的随机森林/AdaBoost，等到有经验再来描述。

----

reference:

1. Andrew Ng: coursera course
2. machine learning in action 
